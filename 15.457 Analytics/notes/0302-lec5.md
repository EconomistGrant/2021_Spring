# Multiple Linear Regression
first feature as all-1: intercept!
X n * p       \beta p * 1   \epsilon n*1
row: all feature of an observation

how to interpret coefficient: partial derivative
sigma_j * beta_j: one std in feature j: whats going to happen in y?

Assumption: 
1. Linearity
2. X is an n*p matrix with rank p
3. Exogenity of independent variables: E[epsilon_i|X] = 0 (when data iid->)
4. Homoscedaticity and nonautocorrelation E[epsilon * epsilon^T] = sigma^2 I(no cross terms, only diagonal)

Intuition of 4: if alpha is large, variance is large also still epsilon mean 0

minimize epsilon' epsilon = (Y-Xb)' (Y-Xb) = ...
FOC(b) = 0 -> (X'X) b = X'Y

b = inv(X'X) X'Y = b + inv(X'X) (X'epsilon) = b + inv(X'X/n) (X'e / n)

$var(\hat{beta}|x) = sigma^2 (X'X)^{-1}$

CAPM: Var(beta_1) = sigma^2 / (n-1)sigma_x^2
sigma, sigma_x cancels out -> higher frequency better
however, the assumption that iid will be violated in high frequency

intuition from Var(beta): larger X'X will be better! wide range of x

# BLUE
Best: minimum var
Linear: in Y
Unbiased

Note that a biased estimator could have smaller variance...

$\hat{\sigma^2} = RSS / (n-p)$

# Questions
Y = Xb + e (true error)

Y = X $\hat{beta} + \hat{\epsilon}$

b = inv(X'X) X'Y

$x^{\prime} \hat{\epsilon}=x^{\prime} Y-x^{\prime} x\left(x^{\prime} x\right)^{-1} x^{\prime} Y=0$ (by construction!)



