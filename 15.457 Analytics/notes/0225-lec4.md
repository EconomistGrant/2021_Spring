## Bayesian Inference
1. likelihood function: p(x|theta) (possibility of observation given paramter)
2. prior distribution: p(theta) (possible parameter values)

Posterior: p(theta|x) = p(theta) * p(x|theta) / p(x)
p(x) = integrate p(x|theta)*p(theta) d(theta)

focus on numerator, denominator is like a "normalize" thing

reasonable range: sharpe ratio cannot exceed 10, finance theory come in?

### Predictive Distribution
forcast $E[X_{T+1}|X_1,...,X_T]$
past -> p(theta|x) ->p(x_{t+1}|theta)
p25: important derivation

essentially the forecast conditional on past data using models in between
p(xT+1|x) = integral p(xT+1|theta) p(theta|x) dtheta

### Example
prior on mu: p(mu): a normal distribution with two hyper parameters m0 and v0
likelihood: p(x|mu): multiple of every P(R_1|mu), function of mu

updated mean and deviation

for long samples: posterior variance shrinking and goes to 0: you are confindent about the parameter!

m_T = weighted average of m0 and R_bar -> tilted toward R with more data

Monte Hall problem: 
Prior: car at door 1
Posterior: car at door 1 | goat at door 3

## Consisten Estimators based on posterior density
MAP estimator: choose theta to maximize posterior distribution p(theta|x)
MLE: maximize p(x|theta)

MAP p(theta) + MLE
for large T: MLE dominates


## Frequentist perspective:
parameters are viewed as constants
Bayesian: parameters are RV
  
# Fat tail
GSCI return, de-mean
model epsilon t using t distribution (control tail, smaller parameter heavier tail)

MLE estimate v is 5.51: 5-sigma event occues every 3.74 years
normal distribution: every 6922 years!!!
